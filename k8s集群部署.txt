##################
### wirte by gg###
##################
# cat /etc/hosts
192.168.88.141	d1
192.168.88.142	d2
192.168.88.143	d3


install docker
# yum install -y yum-utils device-mapper-persistent-data lvm2
# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# yum install docker-ce
# systemctl enable docker && systemctl start docker


install kubernetes
# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
# setenforce 0
# yum install -y kubelet kubeadm kubectl
# systemctl enable kubelet && systemctl start kubelet


# cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
# sysctl --system


# docker info | grep -i cgroup
# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# systemctl daemon-reload
# systemctl restart kubelet
# systemctl stop firewalld


etcd集群安装
# yum install etcd -y
# cat /etc/etcd/etcd.conf
ETCD_NAME="etcd_d1"
ETCD_DATA_DIR="/var/lib/etcd/d1"
ETCD_LISTEN_PEER_URLS="http://192.168.88.141:2380"									# 监听URL，用于与其他节点通讯 
ETCD_LISTEN_CLIENT_URLS="http://192.168.88.141:2379,http://127.0.0.1:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.88.141:2380"						# 告知集群其他节点的URL，tcp2380端口用于集群通信
ETCD_INITIAL_CLUSTER="etcd_d1=http://192.168.88.141:2380,etcd_d2=http://192.168.88.142:2380,etcd_d3=http://192.168.88.142:2380"	#集群中所有节点
ETCD_INITIAL_CLUSTER_STATE="new"													# 新主机注意修改为existing
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-00"										# 集群的ID 
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.88.141:2379"								# 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求 


# cat /usr/lib/systemd/system/etcd.service 
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\" \
														  --data-dir=\"${ETCD_DATA_DIR}\" \
														  --listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" \
														  --listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\" \
														  --advertise-client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\" \
														  --initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" \
														  --initial-cluster=\"${ETCD_INITIAL_CLUSTER}\" \
														  --initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\""
														  
# etcdctl member list		#isLeader显示那台为主
# etcdctl cluster-health														  

# 添加节点
# etcdctl member add etcd_d3 http://192.168.88.143:2380
# 登入d3，删除/var/lib/etcd/下所有文件，并重启

# 重新加入故障节点
# etcdctl remove [id] && rm -rf /var/lib/etcd/* && etcdctl member add [name] [ip:port] && systemctl restart etcd

# etcdctl member list && echo "---------------" && etcdctl cluster-health

------不需要------
flannel安装
# etcdctl --endpoints="http://192.168.88.141:2379" set /coreos.com/network/config '{ "Network": "172.17.0.0/16", "Backend": {"Type": "vxlan"}}'
# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flanneld-amd64
# nohup ./flanneld-amd64 -etcd-endpoints http://127.0.0.1:2379 &
# ./mk-docker-opts.sh -d /etc/default/docker -c
------不需要------


keepalived安装
# yum install keepalived
# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived

global_defs {
   router_id k8s
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens32
    virtual_router_id 88
    priority 98 
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.88.150
    }
}


d1上执行
# cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 192.168.88.141	#修改ip
etcd:
  endpoints:
  - http://192.168.88.141:2379
  - http://192.168.88.142:2379
  - http://192.168.88.143:2379
networking:
  podSubnet: 10.244.0.0/16			#网段
apiServerCertSANs:
- 192.168.88.150				#vip地址
EOF

# kubeadm init --config=config.yaml 
# mkdir -p $HOME/.kube
# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# chown $(id -u):$(id -g) $HOME/.kube/config
# kubectl get pod --all-namespaces && echo "---------------------" && kubectl get nodes


# kubectl get configmap -n kube-system kube-proxy -o yaml > kube-proxy-cm.yaml
# sed -i 's#server:.*#server: https://192.168.88.150:6443#g' kube-proxy-cm.yaml
# kubectl apply -f kube-proxy-cm.yaml --force
# kubectl delete pod -n kube-system -l k8s-app=kube-proxy

# sed -i 's#server:.*#server: https://192.168.88.150:6443#g' /etc/kubernetes/kubelet.conf
# systemctl restart kubelet

d2、d3执行
# scp -rp /etc/kubernetes/pki/* root@d2:/etc/kubernetes/pki/
# scp -rp /etc/kubernetes/pki/* root@d3:/etc/kubernetes/pki/
# rm -rf /etc/kubernetes/pki/apiserver.*
# kubeadm init --config=config.yaml 

# kubectl get configmap -n kube-system kube-proxy -o yaml > kube-proxy-cm.yaml
# sed -i 's#server:.*#server: https://192.168.88.150:6443#g' kube-proxy-cm.yaml
# kubectl apply -f kube-proxy-cm.yaml --force
# kubectl delete pod -n kube-system -l k8s-app=kube-proxy
# sed -i 's#server:.*#server: https://192.168.88.150:6443#g' /etc/kubernetes/kubelet.conf
# systemctl restart kubelet

token
kubeadm join 192.168.88.141:6443 --token 2c09at.ique78i92apjgbhq --discovery-token-ca-cert-hash sha256:f5ad73decef7621c1442a5dd9f03632d435566a1c0508311afc2fd948cc14489
kubeadm join 192.168.88.142:6443 --token 5x9agn.xuf7vtdud1hqd4o2 --discovery-token-ca-cert-hash sha256:f5ad73decef7621c1442a5dd9f03632d435566a1c0508311afc2fd948cc14489
kubeadm join 192.168.88.143:6443 --token ya6yp1.p0fnixblb75baead --discovery-token-ca-cert-hash sha256:f5ad73decef7621c1442a5dd9f03632d435566a1c0508311afc2fd948cc14489

添加时如果报错，请确认该文件/etc/kubernetes/bootstrap-kubelet.conf 


dashboard
# wet https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
修改
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: nodePort                       #####
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30303                  #####
  selector:
    k8s-app: kubernetes-dashboard

使用https://192.168.88.150:30303/

创建admin
# cat admin-token.yaml 
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile

	
获取admin token login
# kubectl describe secret  -n kube-system $(kubectl get secret -n kube-system | grep admin | awk '{print $1}')
-------弃用-------
# kubectl get secret -n kube-system|grep admin
# kubectl describe secret  -n kube-system  
-------弃用-------